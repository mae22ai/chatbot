import os
import fitz
from pdf2image import convert_from_path
from dotenv import load_dotenv
import google.generativeai as genai
from PIL import Image, ImageOps
import numpy as np
import re
import json
from datetime import datetime
import pytesseract
import argparse

# -----------------------------
# í™˜ê²½ ë³€ìˆ˜ & API ì„¤ì •
# -----------------------------
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PDF_DIR = os.path.join(BASE_DIR, "pdf")
JSON_DIR = os.path.join(BASE_DIR, "json")
TXT_DIR = os.path.join(BASE_DIR, "txt")
REF_TXT_DIR = os.path.join(BASE_DIR, "txt-ref")

USD_TO_KRW = 1390

PRICING = {
    "gemini-1.5-flash": {"input": 0.000018, "output": 0.000036},
    "gemini-1.5-pro": {"input": 0.000125, "output": 0.000375}
}

total_cost_usd = 0.0

# -----------------------------
# í”„ë¡¬í”„íŠ¸
# -----------------------------
BASE_PROMPT = """
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•´ ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ë„ì–´ì“°ê¸°ì™€ ì¤„ë°”ê¿ˆì„ ë¬¸ë§¥ì— ë§ê²Œ ìˆ˜ì •
2. ë¬¸ì¥ë¶€í˜¸ë¥¼ í•œêµ­ì–´ ë§ì¶¤ë²•ì— ë§ê²Œ ìˆ˜ì •
3. í•œìë¡œë§Œ ì¨ ìˆëŠ” ëª¨ë“  í•œìì–´ëŠ” 'í•œêµ­ì–´(æ¼¢å­—)' í˜•íƒœë¡œ ë³€í™˜. ì˜ˆ: 'æ¼¢å­—ê°€ ìˆì—ˆë‹¤.' â†’ 'í•œì(æ¼¢å­—)ê°€ ìˆì—ˆë‹¤.'
4. í•™ìˆ  ë…¼ë¬¸ ê¸°í˜¸, íŠ¹ìˆ˜ë¬¸ìëŠ” ì›í˜• ë³´ì¡´
5. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
6. ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì„ì˜ë¡œ ë‹¤ë“¬ì§€ ë§ê³  ìµœëŒ€í•œ ì›ë¬¸ ë³´ì¡´
7. ë…¼ë¬¸ì˜ ëª©ì°¨ êµ¬ì¡°ë¥¼ ê³ ë ¤í•˜ê³  ë¬¸ë‹¨ êµ¬ì„±ì„ ìœ ì§€
8. ì ˆëŒ€ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ ê²ƒ
9. ì…ë ¥ëœ ëª¨ë“  í˜ì´ì§€ë¥¼, ê° í˜ì´ì§€ë³„ [PAGE X] íƒœê·¸ë¥¼ ìœ ì§€í•œ ì±„ ì „ë¶€ ë³€í™˜í•˜ì—¬ ì¶œë ¥í•  ê²ƒ
10. ë³€í™˜ëœ ê²°ê³¼ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ê³ , ì„¤ëª…Â·í‰ê°€Â·ë¶€ì—° ë¬¸êµ¬ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ

[ê°ì£¼/ë¯¸ì£¼ ì²˜ë¦¬ ê·œì¹™]
- ê°ì£¼/ë¯¸ì£¼ ë²ˆí˜¸ëŠ” ë³¸ë¬¸ì— ê·¸ëŒ€ë¡œ ìœ ì§€ (ì˜ˆ: "...í•˜ì˜€ë‹¤Â¹.")
- ê°ì£¼/ë¯¸ì£¼ ë‚´ìš©ì€ ë³¸ë¬¸ì—ì„œëŠ” ì œê±°í•˜ê³ , ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë³„ë„ ì €ì¥:
    [FOOTNOTE 1] ê°ì£¼ ë‚´ìš©
    [ENDNOTE 1] ë¯¸ì£¼ ë‚´ìš©
- ë‹¨, í•´ë‹¹ ê°ì£¼/ë¯¸ì£¼ê°€ ë³¸ë¬¸ ì´í•´ì— í•„ìˆ˜ì ì¸ ê²½ìš°ì—ëŠ” í•´ë‹¹ ìœ„ì¹˜ì— ê´„í˜¸ ì•ˆì— ë³‘í•©:
    ì˜ˆ: "...í•˜ì˜€ë‹¤(ë‹¤ë§Œ, ~ì— ëŒ€í•´ì„œëŠ” ì¶”ê°€ ë…¼ì˜ê°€ í•„ìš”í•˜ë‹¤)."
- ë³¸ë¬¸ê³¼ ê°ì£¼/ë¯¸ì£¼ ë‚´ìš©ì€ ë¶„ë¦¬í•˜ì—¬ ë°˜í™˜

ì¶œë ¥ í˜•ì‹(í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”):
1) ì „ì²˜ë¦¬ëœ ë³¸ë¬¸
<ë³¸ë¬¸ ë‚´ìš©>

2) ê°ì£¼/ë¯¸ì£¼ ëª©ë¡
<ê°ì£¼/ë¯¸ì£¼ ë‚´ìš©>
"""

REF_PROMPT = """
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ 'ì„œë¡ ', 'ì„ í–‰ì—°êµ¬', 'ê´€ë ¨ì—°êµ¬', 'ì´ë¡ ì  ë°°ê²½', 'ë¬¸í—Œì—°êµ¬', 'ì—°êµ¬ë™í–¥', 'ì—°êµ¬ì‚¬ ì •ë¦¬' ë“±
ê¸°ì¡´ ì—°êµ¬ë¥¼ ì„¤ëª…í•˜ëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.
íŠ¹íˆ ë‹¤ë¥¸ ì—°êµ¬ìë“¤ì˜ ì—°êµ¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê±°ë‚˜ ì–¸ê¸‰í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëª¨ë‘ í¬í•¨í•˜ì—¬ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.
ì˜ˆ: 'í™ê¸¸ë™(2020)ì€ ...ë¼ê³  í•˜ì˜€ë‹¤.'ì™€ ê°™ì€ ë¬¸ì¥ì´ í¬í•¨ëœ ë¬¸ë‹¨ì€ ëª¨ë‘ ì¶”ì¶œ.

ì¡°ê±´:
- í˜ì´ì§€ êµ¬ë¶„ íƒœê·¸([PAGE X])ë¥¼ ì ˆëŒ€ ì‚­ì œí•˜ì§€ ë§ê³  ìœ ì§€
- ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì¹œ ê²½ìš° í˜ì´ì§€ ë²”ìœ„ë¥¼ í•œ ì¤„ë¡œ í‘œì‹œ ê°€ëŠ¥
- ì—°êµ¬ ë°©ë²•, ì‹¤í—˜ ê²°ê³¼, ê²°ë¡ ì€ ì œì™¸
- ì¶”ì¶œ í…ìŠ¤íŠ¸ë„ ë„ì–´ì“°ê¸°Â·ë§ì¶¤ë²•Â·í•œì ë³€í™˜ ì ìš©
- ì ˆëŒ€ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ ê²ƒ
- ì¶”ì¶œëœ ê²°ê³¼ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ê³ , ì„¤ëª…Â·í‰ê°€Â·ë¶€ì—° ë¬¸êµ¬ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ

ì¶œë ¥ í˜•ì‹(í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”):
<ë³¸ë¬¸ ë‚´ìš©>
"""

# -----------------------------
# í† í° / ë¹„ìš© ê³„ì‚°
# -----------------------------
def count_tokens(model_name, text):
    try:
        model = genai.GenerativeModel(model_name)
        return model.count_tokens(text).total_tokens
    except Exception:
        return int(len(re.findall(r"\S+", text)) * 1.3)

def estimate_cost(model_name, input_tokens, output_tokens):
    price_in = PRICING[model_name]["input"] * (input_tokens / 1000)
    price_out = PRICING[model_name]["output"] * (output_tokens / 1000)
    return price_in + price_out

# -----------------------------
# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ + OCR
# -----------------------------
def preprocess_image_for_ocr(image):
    gray = ImageOps.grayscale(image)
    arr = np.array(gray)
    binary = (arr > 180) * 255
    return Image.fromarray(np.uint8(binary))

def ocr_with_page_tags(images):
    text_with_tags = []
    for i, img in enumerate(images, start=1):
        try:
            text = pytesseract.image_to_string(img, lang='kor+eng').strip()
        except Exception:
            text = ""
        text_with_tags.append(f"[PAGE {i}]\n{text}")
    return "\n\n".join(text_with_tags)

# -----------------------------
# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
# -----------------------------
def extract_text_with_page_tags(pdf_path, max_pages):
    doc = fitz.open(pdf_path)
    if max_pages is None:
        max_pages = len(doc)

    texts = []
    for i in range(min(max_pages, len(doc))):
        page_text = doc[i].get_text()
        texts.append(f"[PAGE {i+1}]\n{page_text.strip()}")
    return "\n\n".join(texts)

# -----------------------------
# Gemini í˜¸ì¶œ
# -----------------------------
def gemini_process(text, extract_ref=False, vision_images=None):
    global total_cost_usd
    model_name = "gemini-1.5-pro" if vision_images is None else "gemini-1.5-flash"

    prompt = (REF_PROMPT if extract_ref else BASE_PROMPT) + "\n\ní…ìŠ¤íŠ¸:\n" + text

    input_tokens = count_tokens(model_name, prompt)
    try:
        model = genai.GenerativeModel(model_name)
        if vision_images:
            response = model.generate_content([prompt] + vision_images)
        else:
            response = model.generate_content(prompt)
    except Exception as e:
        print(f"âŒ Gemini API Error: {e}")
        return "", model_name, 0, 0, 0.0

    output_text = (response.text or "").strip()
    output_tokens = count_tokens(model_name, output_text)
    cost = estimate_cost(model_name, input_tokens, output_tokens)
    total_cost_usd += cost

    return output_text, model_name, input_tokens, output_tokens, cost

# -----------------------------
# í˜ì´ì§€ ë²”ìœ„ ì¶”ì¶œ
# -----------------------------
def parse_page_ranges_from_ref_text(ref_text):
    pages_found = sorted({int(p) for p in re.findall(r"\[PAGE (\d+)\]", ref_text)})
    if not pages_found:
        return []
    ranges, start, prev = [], pages_found[0], pages_found[0]
    for p in pages_found[1:]:
        if p == prev + 1:
            prev = p
        else:
            ranges.append((start, prev))
            start = prev = p
    ranges.append((start, prev))
    return [f"{s}-{e}" if s != e else f"{s}" for s, e in ranges]

# -----------------------------
# PDF ìœ í˜• íŒë³„
# -----------------------------
def is_text_based_pdf(pdf_path, min_text_len=100):
    try:
        doc = fitz.open(pdf_path)
        total_text_len = 0
        for page in doc:
            total_text_len += len(page.get_text().strip())
            if total_text_len >= min_text_len:
                return True
        return False
    except Exception:
        return False

# -----------------------------
# PDF ì²˜ë¦¬
# -----------------------------
def process_pdf(pdf_path, max_pages=20, force=False):
    rel_path = os.path.relpath(pdf_path, PDF_DIR)
    rel_no_ext = os.path.splitext(rel_path)[0]

    json_path = os.path.join(JSON_DIR, rel_no_ext + ".json")
    txt_clean_path = os.path.join(TXT_DIR, rel_no_ext + ".txt")
    txt_ref_path = os.path.join(REF_TXT_DIR, rel_no_ext + ".txt")

    os.makedirs(os.path.dirname(json_path), exist_ok=True)
    os.makedirs(os.path.dirname(txt_clean_path), exist_ok=True)
    os.makedirs(os.path.dirname(txt_ref_path), exist_ok=True)

    if not force and os.path.exists(json_path):
        print(f"âš ï¸ {pdf_path} ì´ë¯¸ ì²˜ë¦¬ë¨. ìŠ¤í‚µ.")
        return

    try:
        print(f"ğŸ“„ PDF ë¶„ì„ ì‹œì‘: {pdf_path}")
        text_based = is_text_based_pdf(pdf_path)
        print(f"   â†’ PDF ìœ í˜•: {'í…ìŠ¤íŠ¸ ê¸°ë°˜' if text_based else 'ì´ë¯¸ì§€ ê¸°ë°˜'}")

        if text_based:
            text_with_tags = extract_text_with_page_tags(pdf_path, max_pages)
            print(f"   â†’ ì¶”ì¶œ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:\n{text_with_tags[:300]}...\n")
            cleaned, m1, in1, out1, c1 = gemini_process(text_with_tags, extract_ref=False)
            print(f"   [ë³¸ë¬¸ ì²˜ë¦¬] ëª¨ë¸: {m1}, ì…ë ¥í† í°: {in1}, ì¶œë ¥í† í°: {out1}, ë¹„ìš©: ${c1:.6f}")
            ref, m2, in2, out2, c2 = gemini_process(text_with_tags, extract_ref=True)
            print(f"   [ì„ í–‰ì—°êµ¬ ì¶”ì¶œ] ëª¨ë¸: {m2}, ì…ë ¥í† í°: {in2}, ì¶œë ¥í† í°: {out2}, ë¹„ìš©: ${c2:.6f}")
        else:
            images = convert_from_path(pdf_path, dpi=300, first_page=1, last_page=max_pages)
            processed_images = [preprocess_image_for_ocr(img) for img in images]
            ocr_text = ocr_with_page_tags(processed_images)
            print(f"   â†’ OCR ê²°ê³¼ ìƒ˜í”Œ:\n{ocr_text[:300]}...\n")
            cleaned, m1, in1, out1, c1 = gemini_process(ocr_text, extract_ref=False, vision_images=processed_images)
            print(f"   [ë³¸ë¬¸ ì²˜ë¦¬] ëª¨ë¸: {m1}, ì…ë ¥í† í°: {in1}, ì¶œë ¥í† í°: {out1}, ë¹„ìš©: ${c1:.6f}")
            ref, m2, in2, out2, c2 = gemini_process(ocr_text, extract_ref=True, vision_images=processed_images)
            print(f"   [ì„ í–‰ì—°êµ¬ ì¶”ì¶œ] ëª¨ë¸: {m2}, ì…ë ¥í† í°: {in2}, ì¶œë ¥í† í°: {out2}, ë¹„ìš©: ${c2:.6f}")
    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {pdf_path} | {e}")
        return

    ref_page_ranges = parse_page_ranges_from_ref_text(ref)
    result_json = {
        "metadata": {
            "file_name": os.path.basename(pdf_path),
            "relative_path": rel_path,
            "is_text_based": text_based,
            "pages_processed": max_pages,
            "ref_page_ranges": ref_page_ranges,
            "timestamp": datetime.now().isoformat()
        },
        "cost_info": {
            "total_usd": round(c1 + c2, 6),
            "total_krw": round((c1 + c2) * USD_TO_KRW, 0),
            "details": {
                "cleaned_text": {"model": m1, "input_tokens": in1, "output_tokens": out1, "cost_usd": c1},
                "ref_text": {"model": m2, "input_tokens": in2, "output_tokens": out2, "cost_usd": c2}
            }
        },
        "cleaned_text": cleaned,
        "ref_text": ref
    }

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result_json, f, ensure_ascii=False, indent=2)
    with open(txt_clean_path, "w", encoding="utf-8") as f:
        f.write(cleaned or "")
    with open(txt_ref_path, "w", encoding="utf-8") as f:
        f.write(ref or "")

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {json_path}")
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {txt_clean_path}")
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {txt_ref_path}")
    print(f"ğŸ’° ì´ PDF ë¹„ìš©: ${(c1+c2):.6f} (~â‚©{(c1+c2)*USD_TO_KRW:,.0f})")

# -----------------------------
# ì‹¤í–‰
# -----------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--all", action="store_true", help="ì „ì²´ í˜ì´ì§€ ì²˜ë¦¬")
    parser.add_argument("--force", action="store_true", help="ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°")
    args = parser.parse_args()

    max_pages = None if args.all else 20
    for root, _, files in os.walk(PDF_DIR):
        for file in files:
            if file.lower().endswith(".pdf"):
                pdf_path = os.path.join(root, file)
                print(f"\n=== Processing: {pdf_path} ===")
                process_pdf(pdf_path, max_pages=max_pages, force=args.force)

    print(f"\nì´ ì˜ˆìƒ ë¹„ìš©: ${total_cost_usd:.6f} (~â‚©{total_cost_usd * USD_TO_KRW:,.0f})")

if __name__ == "__main__":
    main()
